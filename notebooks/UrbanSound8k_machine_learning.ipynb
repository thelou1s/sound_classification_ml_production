{"cells":[{"cell_type":"markdown","metadata":{"id":"cAR2KCeZGVX3"},"source":["# Applying Machine Learning on UrbanSound8k "]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1336,"status":"ok","timestamp":1614940626767,"user":{"displayName":"Luis Wu","photoUrl":"https://lh6.googleusercontent.com/-IX63EOqU68A/AAAAAAAAAAI/AAAAAAAAAvk/HZuqKUBMyB8/s64/photo.jpg","userId":"18373140213077708261"},"user_tz":-480},"id":"GYGyCwKj9g6U","outputId":"dd3fc380-5a78-496b-d8d1-3af2fb91be4a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Mar  5 10:38:09 UTC 2021\n"]}],"source":["!date"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1919,"status":"ok","timestamp":1614940627384,"user":{"displayName":"Luis Wu","photoUrl":"https://lh6.googleusercontent.com/-IX63EOqU68A/AAAAAAAAAAI/AAAAAAAAAvk/HZuqKUBMyB8/s64/photo.jpg","userId":"18373140213077708261"},"user_tz":-480},"id":"Xb_d0Zh5_T7q","outputId":"dc8b94c5-14ea-46a8-9cc9-00199d20af28"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","total 52\n","drwx------ 2 root root 4096 Mar  5 10:32 audio_files\n","drwx------ 2 root root 4096 Feb  8 06:27 features_mfcc_0\n","drwx------ 8 root root 4096 Mar  5 10:32 flask_app\n","drwx------ 6 root root 4096 Mar  5 10:32 .git\n","-rw------- 1 root root  451 Mar  5 10:32 .gitignore\n","drwx------ 2 root root 4096 Mar  5 10:32 images\n","-rw------- 1 root root 1072 Mar  5 10:32 LICENSE\n","drwx------ 2 root root 4096 Mar  5 10:10 my_features_mfcc\n","drwx------ 2 root root 4096 Feb  7 07:21 my_features_mfcc_0\n","drwx------ 2 root root 4096 Feb 26 06:09 my_features_mfcc_1\n","drwx------ 2 root root 4096 Mar  5 10:32 notebooks\n","-rw------- 1 root root 9441 Mar  5 10:32 README.md\n","drwx------ 4 root root 4096 Jan 30 07:34 UrbanSound8K\n","/content\n","total 20\n","drwxr-xr-x 1 root root 4096 Mar  5 07:32 .\n","drwxr-xr-x 1 root root 4096 Mar  5 07:19 ..\n","drwxr-xr-x 4 root root 4096 Mar  1 14:35 .config\n","drwx------ 5 root root 4096 Mar  5 07:32 drive\n","drwxr-xr-x 1 root root 4096 Mar  1 14:35 sample_data\n"]}],"source":["# 挂载谷歌网盘\r\n","# Colaboratory: Can I access to my Google drive folder and file? \r\n","# https://stackoverflow.com/questions/47744131/colaboratory-can-i-access-to-my-google-drive-folder-and-file\r\n"," \r\n","from google.colab import drive\r\n","drive.mount('/content/drive')\r\n","\r\n","# 测试谷歌网盘\r\n","base_path = '/content/drive/MyDrive/dataset/sound_classification_ml_production/'\r\n","\r\n","!pwd\r\n","!ls -la /content/drive/MyDrive/dataset/sound_classification_ml_production\r\n","!pwd\r\n","!ls -la ."]},{"cell_type":"markdown","metadata":{"id":"USPAMetFGQ7O"},"source":["## Install Packages\n","\n","We install: \n","- Machine learning libraries: `Keras`, `sklearn`\n","- Audio processing: `librosa`\n","- Plots: `Plotly`, `matplotlib`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sefSvCUtzKfx","outputId":"6fdbb60c-e63c-41ca-9e37-ea9f8d665f80"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: numpy\u003e=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n","Requirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied: pytz\u003e=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003e=2.7.3-\u003epandas) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (54.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n","Requirement already satisfied: joblib\u003e=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-\u003esklearn) (1.0.1)\n","Requirement already satisfied: scipy\u003e=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-\u003esklearn) (1.4.1)\n","Requirement already satisfied: numpy\u003e=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-\u003esklearn) (1.19.5)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.0)\n","Requirement already satisfied: decorator\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: scikit-learn!=0.19.0,\u003e=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.22.2.post1)\n","Requirement already satisfied: joblib\u003e=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n","Requirement already satisfied: scipy\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n","Requirement already satisfied: audioread\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n","Requirement already satisfied: numba\u003e=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n","Requirement already satisfied: resampy\u003e=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n","Requirement already satisfied: soundfile\u003e=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n","Requirement already satisfied: numpy\u003e=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.19.5)\n","Requirement already satisfied: pooch\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.3.0)\n","Requirement already satisfied: llvmlite\u003c0.35,\u003e=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba\u003e=0.43.0-\u003elibrosa) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba\u003e=0.43.0-\u003elibrosa) (54.0.0)\n","Requirement already satisfied: six\u003e=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy\u003e=0.2.2-\u003elibrosa) (1.15.0)\n","Requirement already satisfied: cffi\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile\u003e=0.9.0-\u003elibrosa) (1.14.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch\u003e=1.0-\u003elibrosa) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch\u003e=1.0-\u003elibrosa) (2.23.0)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch\u003e=1.0-\u003elibrosa) (1.4.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi\u003e=1.0-\u003esoundfile\u003e=0.9.0-\u003elibrosa) (2.20)\n","Requirement already satisfied: pyparsing\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003epooch\u003e=1.0-\u003elibrosa) (2.4.7)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epooch\u003e=1.0-\u003elibrosa) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epooch\u003e=1.0-\u003elibrosa) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epooch\u003e=1.0-\u003elibrosa) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epooch\u003e=1.0-\u003elibrosa) (2020.12.5)\n"]}],"source":["!pip install pandas\n","!pip install setuptools\n","!pip install numpy\n","!pip install sklearn\n","!pip install librosa\n","!pip install plotly\n","!pip install matplotlib\n","!pip install pillow\n","!pip install keras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MrVjHhS40MZM"},"outputs":[],"source":["import os\n","import time\n","import librosa\n","import zipfile\n","import numpy as np\n","import pandas as pd\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","from PIL import Image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6Ikg2sdn36H0"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 55\n","drwx------ 13 root root  4096 Jan 30 07:34 audio\n","-rw-------  1 root root 15364 May 19  2014 .DS_Store\n","-rw-------  1 root root 26155 May 19  2014 FREESOUNDCREDITS.txt\n","drwx------  2 root root  4096 May 28  2014 metadata\n","-rw-------  1 root root  4932 Jun  3  2014 UrbanSound8K_README.txt\n"]}],"source":["# Unzip dataset\n","# !wget -P /content/drive/MyDrive/dataset/sound_classification_ml_production https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz \n","# !mkdir /content/drive/MyDrive/dataset/sound_classification_ml_production/UrbanSound8K\n","# !tar -xzf UrbanSound8K.tar.gz -C /content/drive/MyDrive/dataset/sound_classification_ml_production/UrbanSound8K\n","# !rm urban8k.tgz\n","\n","!ls -la /content/drive/MyDrive/dataset/sound_classification_ml_production/UrbanSound8K\n","# !mv /content/drive/MyDrive/dataset/sound_classification_ml_production/UrbanSound8K.tar.gz .\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uOTrMVSjIy-A"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","total 20\n","drwxr-xr-x 1 root root 4096 Mar  5 07:32 .\n","drwxr-xr-x 1 root root 4096 Mar  5 07:19 ..\n","drwxr-xr-x 4 root root 4096 Mar  1 14:35 .config\n","drwx------ 5 root root 4096 Mar  5 07:32 drive\n","drwxr-xr-x 1 root root 4096 Mar  1 14:35 sample_data\n"]}],"source":["!pwd \u0026 ls -la\r\n","\r\n","# !mkdir /content/drive/MyDrive/dataset/sound_classification_ml_production/features_mfcc\r\n","# !cp ./features_mfcc/* /content/drive/MyDrive/dataset/sound_classification_ml_production/features_mfcc"]},{"cell_type":"markdown","metadata":{"id":"wOx82BdSom6q"},"source":["## Design Choices and Models"]},{"cell_type":"markdown","metadata":{"id":"7YbZ0ZqyosFi"},"source":["After analysing the dataset and spending a bit of time reading about state-of-the-art on audio signal classification, and some of my [previous work](https://github.com/jsalbert/Music-Genre-Classification-with-Deep-Learning) I have made the following design choices and proposals:\n","\n","Train a Convolutional Neural Network and use either MFCCs, STFT or Mel-Spectogram as input. \n","\n","- As the audios duration range from 0 to 4s, I pad the spectogram generated, to make all the audios of equal length. \n","\n","Feature options:\n","\n","- Using MFCCs as features:\n","  - It is usual to compute the first 13 MFCCs, their derivatives and second derivatives and use it as features.\n","  - Or it is also usual to use 40 MFCCs as it is the Librosa default.\n","\n","- Using the STFT as features:\n","  - Contains less human processing than MFCCs and Mel-Spectogram, the CNN could learn other filters rather than the representations designed by humans.\n","\n","- Using Mel-Spectogram as features:\n","  - A transformation applied on the STFT that approximates how humans perceive the sound. Less human engineered than MFCCs but a bit more than STFT. \n","\n","My first choice would be using STFT and Mel-Spectogram as it looks that CNNs could take more advantage of the frequency-temporal structure but due to **computational resources** and limited time I will show the use **MFCCs** as features as they are much more memory efficient. \n"," "]},{"cell_type":"markdown","metadata":{"id":"ctXVt_OEsrie"},"source":["## Dataset Preprocessing and Splits"]},{"cell_type":"markdown","metadata":{"id":"J728zt898gNW"},"source":["I load all the audio data using Librosa and the default sample rate of 22050Hz. This design decision is based on \n","([Source]((https://librosa.org/blog/2019/07/17/resample-on-load/#Okay...-but-why-22050-Hz?--Why-not-44100-or-48000?))) and in further experiments different sample rates could be tried. \n","\n","\u003e Humans can hear up to around 20000 Hz, it's possible to successfully analyze music and speech data at much lower rates without sacrificing much. The highest pitches we usually care about detecting are around C9≈8372 Hz, well below the 11025 cutoff implied by fs=22050.\n","\n","By default Librosa will load the audio in mono, giving us 1 channel.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cPbaPrONsoiY"},"outputs":[],"source":["# FeatureExtractor class including librosa audio processing functions\n","class FeatureExtractor:\n","    def __init__(self, csv_file):\n","        self.csv_file = csv_file\n","        self.max_audio_duration = 4\n","        self.dataset_df = self._create_dataset(csv_file)\n","    \n","    @staticmethod\n","    def _create_dataset(csv_file):\n","        \"\"\"\n","        Args:\n","            dataset_path: path with the .wav files after unzipping\n","        Returns: A pandas dataframe with the list of files and labels (`filenames`, `labels`)\n","        \"\"\"\n","        dataset_df = pd.read_csv(csv_file)\n","        filepaths = []\n","        for i, row in dataset_df.iterrows():\n","            filepaths.append(os.path.join('UrbanSound8K/audio', 'fold'+str(row['fold']), row['slice_file_name']))\n","        dataset_df['filepath'] = filepaths\n","        return dataset_df\n","\n","    @staticmethod\n","    def _compute_max_pad_length(max_audio_length, sample_rate=22050, n_fft=2048, hop_length=512):\n","        dummy_file = np.random.random(max_audio_length*sample_rate)\n","        stft = librosa.stft(dummy_file, n_fft=n_fft, hop_length=hop_length)\n","        # Return an even number for CNN computation purposes\n","        if stft.shape[1] % 2 != 0:\n","            return stft.shape[1] + 1\n","        return stft.shape[1]\n","\n","    def compute_save_features(self, \n","                        mode='mfcc', \n","                        sample_rate=22050,\n","                        n_fft=2048,\n","                        hop_length=512,\n","                        n_mfcc=40,\n","                        output_path='features',\n","                        deltas=False\n","                        ):\n","        dataset_features = []\n","        max_pad = self._compute_max_pad_length(self.max_audio_duration, \n","                                               sample_rate=sample_rate, \n","                                               n_fft=n_fft,\n","                                               hop_length=hop_length)\n","        print('Max Padding = ', max_pad)\n","        \n","        if not os.path.exists(output_path):\n","            print('Creating output folder: ', output_path)\n","            os.makedirs(output_path)\n","        else:\n","            print('Output folder already existed')\n","            \n","        print('Saving features in ', output_path)\n","        i = 0\n","        t = time.time()\n","        \n","        features_path = []\n","        for relative_filepath in self.dataset_df['filepath']:\n","            filepath = base_path + relative_filepath;\n","            print('compute_save_features, filepath = ' + str(filepath))\n","\n","            if i % 100 == 0:\n","                print('{} files processed in {}s'.format(i, time.time() - t))\n","\n","            print('compute_save_features, librosa.load, filepath = ' + str(filepath))\n","            audio_file, sample_rate = librosa.load(filepath, sr=sample_rate, res_type='kaiser_fast')\n","            if mode == 'mfcc':\n","                audio_features = self.compute_mfcc(audio_file, sample_rate, n_fft, hop_length, n_mfcc, deltas)  \n","            elif mode == 'stft':\n","                audio_features = self.compute_stft(audio_file, sample_rate, n_fft, hop_length)\n","            elif mode == 'mel-spectogram':\n","                audio_features = self.compute_mel_spectogram(audio_file, sample_rate, n_fft, hop_length)\n","            \n","            audio_features = np.pad(audio_features, pad_width=((0, 0), (0, max_pad - audio_features.shape[1])))\n","            print('compute_save_features, audio_features = ' + str(type(audio_features)) + ', ' + str(audio_features.shape))\n","\n","            # here save .npy feature as shape(39, 174, 1)\n","            npy_path = os.path.join(output_path, filepath.split('/')[-1].replace('wav', 'npy'))\n","            print('compute_save_features, npy_path = ' + str(npy_path))\n","            print('compute_save_features, audio_features.shape = ' + str(audio_features.shape))\n","            tmp_audio_features = audio_features.reshape(audio_features.shape[0], audio_features.shape[1], 1)\n","            print('compute_save_features, tmp_audio_features.shape = ' + str(tmp_audio_features.shape))\n","            np.save(npy_path, tmp_audio_features)\n","            \n","            save_path = os.path.join(output_path, filepath.split('/')[-1].replace('wav', 'png'))\n","            print('compute_save_features, save_features, save_path = ' + str(save_path))\n","            print('compute_save_features, audio_features.shape = ' + str(audio_features.shape))\n","            self.save_features(audio_features, save_path)\n","            features_path.append(save_path)\n","            i+=1\n","        self.dataset_df['features_path'] = features_path\n","        return self.dataset_df\n","    \n","    @staticmethod\n","    def save_features(audio_features, filepath):\n","        image = Image.fromarray(audio_features)\n","        # To grayscale\n","        image = image.convert(\"L\")\n","        image.save(filepath)\n","        print('save_features, filepath = ' + str(filepath))\n","\n","    @staticmethod\n","    def compute_mel_spectogram(audio_file, sample_rate, n_fft, hop_length):\n","        return librosa.feature.melspectrogram(audio_file,\n","                                              sr=sample_rate, \n","                                              n_fft=n_fft,\n","                                              hop_length=hop_length)\n","    @staticmethod\n","    def compute_stft(audio_file, sample_rate, n_fft, hop_length):\n","        return librosa.stft(audio_file, n_fft=n_fft, hop_length=hop_length)\n","    \n","    @staticmethod\n","    def compute_mfcc(audio_file, sample_rate, n_fft, hop_length, n_mfcc, deltas=False):\n","        mfccs = librosa.feature.mfcc(audio_file,\n","                                    sr=sample_rate, \n","                                    n_fft=n_fft,\n","                                    n_mfcc=n_mfcc,\n","                                    )\n","        # Change mode from interpolation to nearest\n","        if deltas:\n","          delta_mfccs = librosa.feature.delta(mfccs, mode='nearest')\n","          delta2_mfccs = librosa.feature.delta(mfccs, order=2, mode='nearest')\n","          return np.concatenate((mfccs, delta_mfccs, delta2_mfccs))\n","        return mfccs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Bzfg9qYAH0NC"},"outputs":[],"source":["# Create dataset and extract features\n","fe = FeatureExtractor(base_path + 'UrbanSound8K/metadata/UrbanSound8K.csv')"]},{"cell_type":"markdown","metadata":{"id":"B4wZmY8n8od-"},"source":["Access to disc and librosa loading of audio files is very slow on colab Notebook (30-40 min) we could load the pre-computed features instead."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9fa-rfjH9rn"},"outputs":[],"source":["# Uncomment and run to compute and save features on the colab notebook\n","dataset_df = fe.compute_save_features(mode='mfcc', n_mfcc=13, output_path=base_path + 'my_features_mfcc', deltas=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q9x6AFuuuQvb"},"outputs":[],"source":["# Unzip features\n","# !wget -P /content/drive/MyDrive/dataset/sound_classification_ml_production --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download\u0026confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download\u0026id=1BU2B5EcbfyGBIOkB5YC44hpzPpuqw43H' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\u0026id=1BU2B5EcbfyGBIOkB5YC44hpzPpuqw43H\" -O features_mfcc.zip \u0026\u0026 rm -rf /tmp/cookies.txt\n","# !unzip -q /content/drive/MyDrive/dataset/sound_classification_ml_production/features_mfcc.zip\n","# !rm features_mfcc.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VI_RjDQLBmpb"},"outputs":[],"source":["# Download dataset.json file\n","# !wget -P /content/drive/MyDrive/dataset/sound_classification_ml_production --no-check-certificate \"https://docs.google.com/uc?export=download\u0026id=1pzSvGYaBXghLQFTZxlSex-Ts3T4B0X4C\" -O dataset.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Veb-Zxb5KEJD"},"outputs":[],"source":["dataset_path = base_path + 'dataset.json'\r\n","print(dataset_path)\r\n","dataset_df = pd.read_json(dataset_path)"]},{"cell_type":"markdown","metadata":{"id":"giVHke5JwwYh"},"source":["For the purpose of this experiment we will load all the data in memory and process it in minibatches. If we had computational resources and more time we could create Dataloader objects that would allow to perform many other operations as Data Augmentation and iterate faster. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpWgHLnbI8g-"},"outputs":[],"source":["tmp = []\n","for relative_feature_path in dataset_df['features_path']:\n"," \n","  feature_path = base_path + relative_feature_path.replace('features_mfcc', 'my_features_mfcc')\n","  print('feature_path = ' + str(feature_path))\n"," \n","  tmp.append(feature_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMMTEYMoqks_"},"outputs":[],"source":["# Numpy：numpy与image互转 https://blog.csdn.net/weixin_40522801/article/details/106490005\r\n","\r\n","dataset_df['features'] = [np.asarray(np.load(base_path + feature_path.replace('features_mfcc', 'my_features_mfcc'))) for feature_path in dataset_df['features_path']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHyfHqtKzxAE"},"outputs":[],"source":["from keras.utils import to_categorical\n","dataset_df['labels_categorical'] = [to_categorical(label, 10) for label in dataset_df['classID']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HfDBipkQz1RF"},"outputs":[],"source":["dataset_df.head()"]},{"cell_type":"markdown","metadata":{"id":"3pQN4JBIxWeP"},"source":["We are going to create splits for the train, validation and test sets of our dataset. \n","For the purpose of the experiment and to make it quick we will use the sklearn function `train_test_split`, two times. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUCN2TwZQMDs"},"outputs":[],"source":["dataset_df_tolist = dataset_df['features'].tolist()\r\n","np_array = np.array(dataset_df_tolist)\r\n","print('dataset_df_tolist = ' + str(dataset_df_tolist))\r\n","print('np_array = ' + str(np_array))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5-k-OKxXqkwh"},"outputs":[],"source":["# Split the dataset \n","from sklearn.model_selection import train_test_split \n","\n","# Add one dimension for the channel\n","X = np.array(dataset_df['features'].tolist())\n","y = np.array(dataset_df['labels_categorical'].tolist())\n","\n","# As there is unbalance for some classes I am going to stratify it so we have the same proportion in train/test\n","X_train, X_test, Y_train, Y_test = train_test_split(X, \n","                                                    y, \n","                                                    test_size=0.30, \n","                                                    random_state=1, \n","                                                    stratify=y)\n","# Create validation and test\n","X_test, X_val, Y_test, Y_val = train_test_split(X_test, \n","                                                Y_test, \n","                                                test_size=0.5, \n","                                                random_state=1, \n","                                                stratify=Y_test)\n","\n","print(X_train.shape, X_val.shape, X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"3mbA_78g6iUa"},"source":["## Machine Learning Model"]},{"cell_type":"markdown","metadata":{"id":"0UXoQEZ16pjr"},"source":["### Model Design"]},{"cell_type":"markdown","metadata":{"id":"M-sygDIE29Og"},"source":["We are going to create a **Fully Convolutional Network** Model using Keras running over Tensorflow with a few layers. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ko1n0Tla3n1y"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Input, Dense, Dropout, Activation, Flatten\n","from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D"]},{"cell_type":"markdown","metadata":{"id":"y589P2Qm_AuG"},"source":["As our images are rectangular in shape (y axis is MFCC, x axis is time), instead of using square filters (as usual) we are going to make them rectangular so they can learn better the correlation of the MFCCs with the temporal dimension. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FdM5w8r1kMgs"},"outputs":[],"source":["# FCN Model\n","def create_model(num_classes=10, input_shape=None, dropout_ratio=None):\n","    model = Sequential()\n","    if input_shape is None:\n","        model.add(Input(shape=(None, None, 1)))\n","    else:\n","        model.add(Input(shape=input_shape))\n","    model.add(Conv2D(filters=16, kernel_size=(2, 4), activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 3)))\n","    model.add(Conv2D(filters=32, kernel_size=(2, 4), activation='relu'))\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Conv2D(filters=64, kernel_size=(2, 4), activation='relu'))\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Conv2D(filters=128, kernel_size=(2, 4), activation='relu'))\n","    model.add(GlobalAveragePooling2D())\n","    if dropout_ratio is not None:\n","        model.add(Dropout(dropout_ratio))\n","    # Add dense linear layer\n","    model.add(Dense(num_classes, activation='softmax'))\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"Qj73VymN-ueo"},"source":["As it is a multi classification problem we will use the **Categorical Cross Entropy loss**. As optimizer we will use the Keras implementation of **Adam** with the default hyperparameters values. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vaj8BrhD4cyH"},"outputs":[],"source":["# Create and compile the model\n","fcn_model = create_model(input_shape=X_train.shape[1:])\n","fcn_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n","fcn_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"tWTJXf4B6uNW"},"source":["### Model training and evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBxKnyeo64TG"},"outputs":[],"source":["from keras.models import load_model\n","from keras.callbacks import ModelCheckpoint "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTT0_wO27AFy"},"outputs":[],"source":["!mkdir /content/drive/MyDrive/dataset/sound_classification_ml_production/saved_models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1hJEl2f5JLI"},"outputs":[],"source":["def train_model(model, X_train, Y_train, X_val, Y_val, epochs, batch_size, callbacks):\n","    model.fit(X_train, \n","              Y_train, \n","              batch_size=batch_size, \n","              epochs=epochs, \n","              validation_data=(X_val, Y_val), \n","              callbacks=callbacks, verbose=1)\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"ZfzcdmYtETt3"},"source":["We will create a checkpoint for **early stopping**, so we will select the model that performs better on the validation set. \n","\n","Creating a function to train the model will allow us to perform hyperparameter tuning faster. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nt9Q7KQ67WmV"},"outputs":[],"source":["checkpointer = ModelCheckpoint(filepath=base_path + 'saved_models/best_fcn.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","callbacks = [checkpointer]\n","\n","# Hyper-parameters\n","epochs = 100\n","batch_size = 256"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5IB-m4c17E7e"},"outputs":[],"source":["# Train the model\n","model = train_model(model=fcn_model,\n","                    X_train=X_train,\n","                    X_val=X_val,\n","                    Y_train=Y_train,\n","                    Y_val=Y_val,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7lYwwrwDGey"},"outputs":[],"source":["# Load the best model\n","best_model = load_model(base_path + 'saved_models/best_fcn.hdf5')\n","\n","# !ls /content/drive/MyDrive/dataset/sound_classification_ml_production/saved_models"]},{"cell_type":"markdown","metadata":{"id":"AvkVp34YEwRG"},"source":["Looks like the model has overfitted to the training data towards the end of the training. We have selected the model that performed better on the validation set, saved by the checkpoint. The similarity between validation and test score tells us that our training methodology is correct and that our validation set is a good estimator of testing performance. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5a-BhlFDGj1"},"outputs":[],"source":["# Evaluating the model on the training and testing set\n","score = best_model.evaluate(X_train, Y_train, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = best_model.evaluate(X_val, Y_val, verbose=0)\n","print(\"Validation Accuracy: \", score[1])\n","\n","score = best_model.evaluate(X_test, Y_test, verbose=0)\n","print(\"Testing Accuracy: \", score[1])"]},{"cell_type":"markdown","metadata":{"id":"B5wvOmYNBcZT"},"source":["We see that there has been overfitting so we could train another model adding dropout before the last layer to add more regularization.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCA2qchv5JN9"},"outputs":[],"source":["# We add a dropout ratio of 0.25\n","fcn_model = create_model(input_shape=X_train.shape[1:], dropout_ratio=0.5)\n","fcn_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n","fcn_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F3YhwGSN5JRd"},"outputs":[],"source":["checkpointer = ModelCheckpoint(filepath=base_path + 'saved_models/best_fcn_dropout.hdf5', monitor='val_accuracy',\n","                               verbose=1, save_best_only=True)\n","callbacks = [checkpointer]\n","\n","model = train_model(model=fcn_model,\n","                    X_train=X_train,\n","                    X_val=X_val,\n","                    Y_train=Y_train,\n","                    Y_val=Y_val,\n","                    epochs=200,\n","                    batch_size=256,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTyotHRpAMbB"},"outputs":[],"source":["best_model = load_model(base_path + 'saved_models/best_fcn_dropout.hdf5')\r\n","\r\n","# !ls /content/drive/MyDrive/dataset/sound_classification_ml_production/saved_models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UK4BTzRB_1Q"},"outputs":[],"source":["# Evaluating the model on the training and testing set\n","score = best_model.evaluate(X_train, Y_train, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = best_model.evaluate(X_val, Y_val, verbose=0)\n","print(\"Validation Accuracy: \", score[1])\n","\n","score = best_model.evaluate(X_test, Y_test, verbose=0)\n","print(\"Testing Accuracy: \", score[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inagrMP3A4rB"},"outputs":[],"source":["# Plot a confusion matrix\n","from sklearn import metrics\n","Y_pred = best_model.predict(X_test)\n","matrix = metrics.confusion_matrix(Y_test.argmax(axis=1), Y_pred.argmax(axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzTmmTg6A4oy"},"outputs":[],"source":["# Confusion matrix code (from https://github.com/triagemd/keras-eval/blob/master/keras_eval/visualizer.py)\n","def plot_confusion_matrix(cm, concepts, normalize=False, show_text=True, fontsize=18, figsize=(16, 12),\n","                          cmap=plt.cm.coolwarm_r, save_path=None, show_labels=True):\n","    '''\n","    Plot confusion matrix provided in 'cm'\n","    Args:\n","        cm: Confusion Matrix, square sized numpy array\n","        concepts: Name of the categories to show\n","        normalize: If True, normalize values between 0 and ones. Not valid if negative values.\n","        show_text: If True, display cell values as text. Otherwise only display cell colors.\n","        fontsize: Size of text\n","        figsize: Size of figure\n","        cmap: Color choice\n","        save_path: If `save_path` specified, save confusion matrix in that location\n","    Returns: Nothing. Plots confusion matrix\n","    '''\n","\n","    if cm.ndim != 2 or cm.shape[0] != cm.shape[1]:\n","        raise ValueError('Invalid confusion matrix shape, it should be square and ndim=2')\n","\n","    if cm.shape[0] != len(concepts) or cm.shape[1] != len(concepts):\n","        raise ValueError('Number of concepts (%i) and dimensions of confusion matrix do not coincide (%i, %i)' %\n","                         (len(concepts), cm.shape[0], cm.shape[1]))\n","\n","    plt.rcParams.update({'font.size': fontsize})\n","\n","    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    if normalize:\n","        cm = cm_normalized\n","\n","    fig = plt.figure(figsize=figsize)\n","    ax = fig.add_subplot(111)\n","    cax = ax.matshow(cm, vmin=np.min(cm), vmax=np.max(cm), alpha=0.8, cmap=cmap)\n","\n","    fig.colorbar(cax)\n","    ax.xaxis.tick_bottom()\n","    plt.ylabel('True label', fontweight='bold')\n","    plt.xlabel('Predicted label', fontweight='bold')\n","\n","    if show_labels:\n","        n_labels = len(concepts)\n","        ax.set_xticklabels(concepts)\n","        ax.set_yticklabels(concepts)\n","        plt.xticks(np.arange(0, n_labels, 1.0), rotation='vertical')\n","        plt.yticks(np.arange(0, n_labels, 1.0))\n","    else:\n","        plt.axis('off')\n","\n","    if show_text:\n","        # http://stackoverflow.com/questions/21712047/matplotlib-imshow-matshow-display-values-on-plot\n","        min_val, max_val = 0, len(concepts)\n","        ind_array = np.arange(min_val, max_val, 1.0)\n","        x, y = np.meshgrid(ind_array, ind_array)\n","        for i, (x_val, y_val) in enumerate(zip(x.flatten(), y.flatten())):\n","            c = cm[int(x_val), int(y_val)]\n","            ax.text(y_val, x_val, c, va='center', ha='center')\n","\n","    if save_path is not None:\n","        plt.savefig(save_path)"]},{"cell_type":"markdown","metadata":{"id":"MX4fa42o1zyW"},"source":["To observe better the performance of the model and the mistakes made between different classes we plot the confusion matrix. \n","\n","In our case accuracy is a good metric because the dataset is mostly balanced but we observed a few classes with less samples (1`car_horn`, `gun_shot` and `siren`), so it will be good to observe the performance on these classes.\n","\n","We can observe that a lot of mistakes are happening between class `children_playing` and class `street_music` so maybe it will be worth it to spend a little bit more time doing analysis and finding what could be the reasons.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WabP5FBC8ph"},"outputs":[],"source":["class_dictionary = {3: 'dog_bark', 2: 'children_playing', 1: 'car_horn', 0: 'air_conditioner', 9: 'street_music', 6: 'gun_shot', 8: 'siren', 5: 'engine_idling', 7: 'jackhammer', 4: 'drilling'}\n","classes = [class_dictionary[key] for key in sorted(class_dictionary.keys())]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIsWLoxGHHOv"},"outputs":[],"source":["plot_confusion_matrix(matrix, classes)"]},{"cell_type":"markdown","metadata":{"id":"12pnVV0xLiBr"},"source":["## Conclusions\n","\n","We can observe a bump of 1-2% in the test set accuracy when introducing dropout as regularization. This shows that it has been a successful addition to our model.\n","\n","There are many things that we can try to improve the model's performance such as:\n","\n","- Hyperparameter tuning:\n","  - Tuning the parameters of feature extraction\n","  - Tuning the network parameters (number of layers, pooling layers, number and filter shape...)\n","  - Tuning the network hyperparameters (Learning rate, optimizer) \n","\n","- Feature extraction:\n","  - Use STFT: The raw spectogram could provide more information to the CNN to learn correlation between frequency and time than the MFCCs.\n","  - Use Mel-Spectogram: The mel-spectogram could provide more information to the CNN to learn correlation between frequency and time than the MFCCs. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mN7nxrk3ZfRt"},"outputs":[],"source":["!ls /content/drive/MyDrive/dataset/audio_classifier_tutorial/UrbanSound8K"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V74NgqbTW5s3"},"outputs":[],"source":["# import matplotlib\r\n","# matplotlib.use('Agg')\r\n","import os\r\n","\r\n","import librosa\r\n","from tensorflow.keras.models import load_model\r\n","import numpy as np\r\n","# from PIL import Image\r\n","# import cv2\r\n","\r\n","#加载模型h5文件\r\n","\r\n","# 读取音频数据\r\n","def load_data(data_path):\r\n","    wav, sr = librosa.load(data_path, sr=16000)\r\n","    intervals = librosa.effects.split(wav, top_db=20)\r\n","    wav_output = []\r\n","    for sliced in intervals:\r\n","        wav_output.extend(wav[sliced[0]:sliced[1]])\r\n","    assert len(wav_output) \u003e= 8000, \"有效音频小于0.5s\"\r\n","    wav_output = np.array(wav_output)\r\n","    ps = librosa.feature.melspectrogram(y=wav_output, sr=sr, hop_length=256).astype(np.float32)\r\n","    ps = ps[np.newaxis, ..., np.newaxis]\r\n","    return ps\r\n","\r\n","load_data_car_119_10785 = load_data('/content/drive/MyDrive/dataset/audio_classifier_tutorial/UrbanSound8K/predict/car_119_10785.wav')\r\n","best_model.predict(load_data_car_119_10785)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FXd8fiS9jHC"},"outputs":[],"source":["!date"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"UrbanSound8k_machine_learning.ipynb","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}